#!/usr/bin/env python3
"""
Script de test pour vÃ©rifier la logique de filtrage par confiance
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.database import get_db
from app.models.database import MLModels, ScreenerResult
from sqlalchemy.orm import Session

def test_confidence_filtering():
    """Test de la logique de filtrage par confiance"""
    
    print("ğŸ” Test de la logique de filtrage par confiance")
    print("=" * 60)
    
    db = next(get_db())
    try:
        # RÃ©cupÃ©rer quelques modÃ¨les rÃ©cents
        recent_models = db.query(MLModels).filter(
            MLModels.is_active == True
        ).limit(5).all()
        
        print(f"ğŸ“Š {len(recent_models)} modÃ¨les rÃ©cents trouvÃ©s")
        
        for model in recent_models:
            print(f"\nğŸ¤– ModÃ¨le: {model.model_name}")
            print(f"   Symbole: {model.symbol}")
            print(f"   Type: {model.model_type}")
            
            # Simuler une prÃ©diction
            confidence = 0.85  # 85% de confiance
            prediction = 1.0   # PrÃ©diction positive
            
            print(f"   PrÃ©diction simulÃ©e: {prediction}")
            print(f"   Confiance simulÃ©e: {confidence:.1%}")
            
            # Test avec diffÃ©rents seuils de confiance
            thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
            
            for threshold in thresholds:
                is_opportunity = prediction == 1.0 and confidence >= threshold
                status = "âœ… OPPORTUNITÃ‰" if is_opportunity else "âŒ RejetÃ©e"
                print(f"   Seuil {threshold:.1%}: {status}")
        
        # VÃ©rifier les rÃ©sultats de screener existants
        print(f"\nğŸ“‹ VÃ©rification des rÃ©sultats de screener existants")
        screener_results = db.query(ScreenerResult).limit(10).all()
        
        print(f"ğŸ“Š {len(screener_results)} rÃ©sultats de screener trouvÃ©s")
        
        for result in screener_results:
            print(f"\nğŸ¯ {result.symbol}")
            print(f"   PrÃ©diction: {result.prediction}")
            print(f"   Confiance: {result.confidence:.1%}")
            print(f"   Rang: #{result.rank}")
            
            # VÃ©rifier si ce rÃ©sultat passerait les filtres
            thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
            
            for threshold in thresholds:
                would_pass = result.prediction == 1.0 and result.confidence >= threshold
                status = "âœ… Passerait" if would_pass else "âŒ RejetÃ©"
                print(f"   Seuil {threshold:.1%}: {status}")
        
        # Statistiques des confiances
        print(f"\nğŸ“Š Statistiques des confiances")
        all_confidences = [r.confidence for r in screener_results]
        
        if all_confidences:
            avg_confidence = sum(all_confidences) / len(all_confidences)
            min_confidence = min(all_confidences)
            max_confidence = max(all_confidences)
            
            print(f"   Confiance moyenne: {avg_confidence:.1%}")
            print(f"   Confiance minimale: {min_confidence:.1%}")
            print(f"   Confiance maximale: {max_confidence:.1%}")
            
            # Compter les opportunitÃ©s par seuil
            for threshold in [0.5, 0.6, 0.7, 0.8, 0.9]:
                count = sum(1 for r in screener_results 
                           if r.prediction == 1.0 and r.confidence >= threshold)
                print(f"   Seuil {threshold:.1%}: {count} opportunitÃ©s")
        
    finally:
        db.close()

def test_prediction_logic():
    """Test de la logique de prÃ©diction"""
    
    print(f"\nğŸ§® Test de la logique de prÃ©diction")
    print("-" * 40)
    
    # Simuler diffÃ©rents scÃ©narios
    scenarios = [
        {"prediction": 1.0, "confidence": 0.95, "threshold": 0.7, "expected": True},
        {"prediction": 1.0, "confidence": 0.65, "threshold": 0.7, "expected": False},
        {"prediction": 0.0, "confidence": 0.95, "threshold": 0.7, "expected": False},
        {"prediction": 1.0, "confidence": 0.75, "threshold": 0.7, "expected": True},
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        prediction = scenario["prediction"]
        confidence = scenario["confidence"]
        threshold = scenario["threshold"]
        expected = scenario["expected"]
        
        result = prediction == 1.0 and confidence >= threshold
        status = "âœ… PASS" if result == expected else "âŒ FAIL"
        
        print(f"   ScÃ©nario {i}: {status}")
        print(f"      PrÃ©diction: {prediction}, Confiance: {confidence:.1%}, Seuil: {threshold:.1%}")
        print(f"      RÃ©sultat: {result}, Attendu: {expected}")

if __name__ == "__main__":
    print("ğŸš€ DÃ©marrage des tests de filtrage par confiance")
    
    test_confidence_filtering()
    test_prediction_logic()
    
    print("\nğŸ Tests terminÃ©s")

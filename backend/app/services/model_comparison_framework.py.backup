"""
Framework de Comparaison de Modèles ML pour le Trading
=====================================================

Ce module fournit un framework complet pour comparer différents modèles
d'apprentissage automatique dans le contexte du trading financier.

Fonctionnalités :
- Comparaison de multiples modèles ML
- Métriques de trading spécialisées
- Validation croisée temporelle
- Visualisation des performances
- Sélection automatique du meilleur modèle
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, date
from dataclasses import dataclass
from abc import ABC, abstractmethod
import joblib
import json
from pathlib import Path

def convert_numpy_types(obj):
    """Convertit les types NumPy en types Python natifs pour la sérialisation"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

# Imports pour les modèles ML
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler

# Imports pour les modèles avancés (optionnels)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("XGBoost non disponible. Installez avec: pip install xgboost")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("LightGBM non disponible. Installez avec: pip install lightgbm")

# Désactiver TensorFlow temporairement à cause du mutex
TENSORFLOW_AVAILABLE = False
print("TensorFlow désactivé temporairement (problème de mutex)")

# try:
#     import tensorflow as tf
#     from tensorflow.keras.models import Sequential
#     from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
#     from tensorflow.keras.optimizers import Adam
#     from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
#     from tensorflow.keras.regularizers import l1_l2
#     TENSORFLOW_AVAILABLE = True
# except ImportError:
#     TENSORFLOW_AVAILABLE = False
#     print("TensorFlow non disponible. Installez avec: pip install tensorflow")
# except Exception as e:
#     TENSORFLOW_AVAILABLE = False
#     print(f"TensorFlow non disponible (erreur mutex): {e}")

# Vérification de la disponibilité de PyTorch avec support Mac M1
PYTORCH_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    
    # Test simple pour vérifier que PyTorch fonctionne
    x = torch.tensor([1.0, 2.0, 3.0])
    y = x * 2
    
    # Vérifier le support MPS pour Mac M1
    if torch.backends.mps.is_available():
        print("PyTorch disponible avec support MPS (Mac M1)")
        DEVICE_TYPE = "mps"
    elif torch.cuda.is_available():
        print("PyTorch disponible avec support CUDA")
        DEVICE_TYPE = "cuda"
    else:
        print("PyTorch disponible (CPU seulement)")
        DEVICE_TYPE = "cpu"
    
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    print("PyTorch non disponible. Installez avec: pip install torch")
except Exception as e:
    PYTORCH_AVAILABLE = False
    print(f"PyTorch non disponible: {e}")

try:
    from sklearn.neural_network import MLPClassifier, MLPRegressor
    NEURAL_NETWORKS_AVAILABLE = True
except ImportError:
    NEURAL_NETWORKS_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class ModelMetrics:
    """Métriques de performance d'un modèle"""
    # Métriques ML classiques
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float
    
    # Métriques de trading
    sharpe_ratio: float
    max_drawdown: float
    total_return: float
    win_rate: float
    profit_factor: float
    
    # Métriques temporelles
    training_time: float
    prediction_time: float
    
    # Métadonnées
    model_name: str
    timestamp: datetime
    parameters: Dict[str, Any]

@dataclass
class TradingResults:
    """Résultats de backtesting pour un modèle"""
    trades: List[Dict[str, Any]]
    equity_curve: List[float]
    daily_returns: List[float]
    sharpe_ratio: float
    max_drawdown: float
    total_return: float
    win_rate: float
    profit_factor: float
    total_trades: int
    winning_trades: int
    losing_trades: int

class BaseModel(ABC):
    """Classe de base pour tous les modèles ML"""
    
    def __init__(self, name: str, **kwargs):
        self.name = name
        self.parameters = kwargs
        self.model = None
        self.is_trained = False
        self.scaler = StandardScaler()
        
    @abstractmethod
    def _create_model(self) -> Any:
        """Créer l'instance du modèle"""
        pass
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'BaseModel':
        """Entraîner le modèle"""
        start_time = datetime.now()
        
        # Normaliser les features
        X_scaled = self.scaler.fit_transform(X)
        
        # Créer et entraîner le modèle
        self.model = self._create_model()
        self.model.fit(X_scaled, y)
        
        self.is_trained = True
        self.training_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"Modèle {self.name} entraîné en {self.training_time:.2f}s")
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Faire des prédictions"""
        if not self.is_trained:
            raise ValueError("Le modèle doit être entraîné avant de faire des prédictions")
        
        start_time = datetime.now()
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        self.prediction_time = (datetime.now() - start_time).total_seconds()
        
        return predictions
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Faire des prédictions probabilistes"""
        if not self.is_trained:
            raise ValueError("Le modèle doit être entraîné avant de faire des prédictions")
        
        X_scaled = self.scaler.transform(X)
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X_scaled)
        else:
            # Pour les modèles sans predict_proba, utiliser decision_function
            if hasattr(self.model, 'decision_function'):
                scores = self.model.decision_function(X_scaled)
                # Convertir en probabilités avec sigmoid
                probabilities = 1 / (1 + np.exp(-scores))
                return np.column_stack([1 - probabilities, probabilities])
            else:
                raise ValueError(f"Le modèle {self.name} ne supporte pas les prédictions probabilistes")

class RandomForestModel(BaseModel):
    """Modèle Random Forest"""
    
    def _create_model(self):
        return RandomForestClassifier(
            n_estimators=self.parameters.get('n_estimators', 100),
            max_depth=self.parameters.get('max_depth', None),
            min_samples_split=self.parameters.get('min_samples_split', 2),
            min_samples_leaf=self.parameters.get('min_samples_leaf', 1),
            random_state=self.parameters.get('random_state', 42),
            n_jobs=self.parameters.get('n_jobs', -1)
        )

class XGBoostModel(BaseModel):
    """Modèle XGBoost"""
    
    def _create_model(self):
        if not XGBOOST_AVAILABLE:
            raise ImportError("XGBoost n'est pas installé")
        
        return xgb.XGBClassifier(
            n_estimators=self.parameters.get('n_estimators', 100),
            max_depth=self.parameters.get('max_depth', 6),
            learning_rate=self.parameters.get('learning_rate', 0.1),
            subsample=self.parameters.get('subsample', 1.0),
            colsample_bytree=self.parameters.get('colsample_bytree', 1.0),
            random_state=self.parameters.get('random_state', 42),
            n_jobs=self.parameters.get('n_jobs', -1)
        )

class LightGBMModel(BaseModel):
    """Modèle LightGBM"""
    
    def _create_model(self):
        if not LIGHTGBM_AVAILABLE:
            raise ImportError("LightGBM n'est pas installé")
        
        return lgb.LGBMClassifier(
            n_estimators=self.parameters.get('n_estimators', 100),
            max_depth=self.parameters.get('max_depth', -1),
            learning_rate=self.parameters.get('learning_rate', 0.1),
            subsample=self.parameters.get('subsample', 1.0),
            colsample_bytree=self.parameters.get('colsample_bytree', 1.0),
            random_state=self.parameters.get('random_state', 42),
            n_jobs=self.parameters.get('n_jobs', -1),
            verbose=-1
        )

class NeuralNetworkModel(BaseModel):
    """Modèle Neural Network"""
    
    def _create_model(self):
        if not NEURAL_NETWORKS_AVAILABLE:
            raise ImportError("Neural Networks ne sont pas disponibles")
        
        return MLPClassifier(
            hidden_layer_sizes=self.parameters.get('hidden_layer_sizes', (100, 50)),
            activation=self.parameters.get('activation', 'relu'),
            solver=self.parameters.get('solver', 'adam'),
            alpha=self.parameters.get('alpha', 0.0001),
            learning_rate=self.parameters.get('learning_rate', 'constant'),
            max_iter=self.parameters.get('max_iter', 1000),
            random_state=self.parameters.get('random_state', 42)
        )

class LSTMModel(BaseModel):
    """Modèle LSTM spécialisé pour les données financières séquentielles"""
    
    def __init__(self, name: str = "LSTM", parameters: Optional[Dict[str, Any]] = None):
        super().__init__(name, **parameters if parameters else {})
        self.scaler = MinMaxScaler()
        self.sequence_length = self.parameters.get('sequence_length', 60)  # 60 jours de lookback
        self.feature_columns = None
        self.is_fitted = False
        
    def _create_model(self):
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow n'est pas disponible. Installez avec: pip install tensorflow")
        
        # Paramètres par défaut optimisés pour les données financières
        lstm_units = self.parameters.get('lstm_units', [128, 64])
        dropout_rate = self.parameters.get('dropout_rate', 0.3)
        learning_rate = self.parameters.get('learning_rate', 0.001)
        l1_reg = self.parameters.get('l1_reg', 0.01)
        l2_reg = self.parameters.get('l2_reg', 0.01)
        
        model = Sequential()
        
        # Première couche LSTM avec return_sequences=True pour les couches suivantes
        model.add(LSTM(
            units=lstm_units[0],
            return_sequences=True,
            input_shape=(self.sequence_length, len(self.feature_columns)) if self.feature_columns else (self.sequence_length, 1),
            kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)
        ))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))
        
        # Couches LSTM supplémentaires
        for i, units in enumerate(lstm_units[1:], 1):
            return_sequences = i < len(lstm_units) - 1
            model.add(LSTM(
                units=units,
                return_sequences=return_sequences,
                kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)
            ))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))
        
        # Couches Dense pour la classification
        model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))
        model.add(Dropout(dropout_rate))
        model.add(Dense(16, activation='relu'))
        model.add(Dropout(dropout_rate / 2))
        
        # Couche de sortie pour classification binaire
        model.add(Dense(1, activation='sigmoid'))
        
        # Compilation avec optimiseur Adam
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def prepare_sequential_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prépare les données pour l'entraînement LSTM en créant des séquences temporelles
        
        Args:
            X: Features (doit être trié par date)
            y: Target (doit être trié par date)
            
        Returns:
            X_sequences: Données séquentielles (samples, timesteps, features)
            y_sequences: Targets correspondants
        """
        # Normaliser les features
        X_scaled = self.scaler.fit_transform(X)
        
        # Créer les séquences
        X_sequences = []
        y_sequences = []
        
        for i in range(self.sequence_length, len(X_scaled)):
            # Séquence de features (lookback window)
            X_sequences.append(X_scaled[i-self.sequence_length:i])
            # Target correspondant
            y_sequences.append(y.iloc[i])
        
        return np.array(X_sequences), np.array(y_sequences)
    
    def fit(self, X: pd.DataFrame, y: pd.Series, validation_split: float = 0.2) -> Dict[str, Any]:
        """
        Entraîne le modèle LSTM avec gestion des séquences temporelles
        
        Args:
            X: Features (doit être trié par date)
            y: Target (doit être trié par date)
            validation_split: Proportion des données pour la validation
            
        Returns:
            Dict avec les métriques d'entraînement
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow n'est pas disponible")
        
        # Stocker les colonnes de features
        self.feature_columns = X.columns.tolist()
        
        # Préparer les données séquentielles
        X_sequences, y_sequences = self.prepare_sequential_data(X, y)
        
        # Créer le modèle
        self.model = self._create_model()
        
        # Callbacks pour améliorer l'entraînement
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=self.parameters.get('patience', 20),
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # Entraînement
        history = self.model.fit(
            X_sequences, y_sequences,
            epochs=self.parameters.get('epochs', 100),
            batch_size=self.parameters.get('batch_size', 32),
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=self.parameters.get('verbose', 1)
        )
        
        self.is_fitted = True
        
        return {
            'history': history.history,
            'best_epoch': len(history.history['loss']),
            'final_loss': history.history['loss'][-1],
            'final_val_loss': history.history['val_loss'][-1],
            'final_accuracy': history.history['accuracy'][-1],
            'final_val_accuracy': history.history['val_accuracy'][-1]
        }
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Prédictions avec le modèle LSTM"""
        if not self.is_fitted:
            raise ValueError("Le modèle doit être entraîné avant de faire des prédictions")
        
        # Normaliser les données avec le scaler entraîné
        X_scaled = self.scaler.transform(X)
        
        # Créer les séquences pour la prédiction
        X_sequences = []
        for i in range(self.sequence_length, len(X_scaled) + 1):
            X_sequences.append(X_scaled[i-self.sequence_length:i])
        
        X_sequences = np.array(X_sequences)
        
        # Prédictions
        predictions = self.model.predict(X_sequences, verbose=0)
        
        # Retourner les prédictions comme probabilités
        return predictions.flatten()
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Probabilités de prédiction"""
        predictions = self.predict(X)
        # Retourner les probabilités pour les deux classes
        return np.column_stack([1 - predictions, predictions])
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        Pour LSTM, on ne peut pas facilement extraire l'importance des features
        comme pour les arbres. On retourne un dictionnaire vide.
        """
        return {}
    
    def save_model(self, filepath: str):
        """Sauvegarde le modèle LSTM et le scaler"""
        if not self.is_fitted:
            raise ValueError("Le modèle doit être entraîné avant d'être sauvegardé")
        
        # Sauvegarder le modèle Keras
        self.model.save(f"{filepath}_model.h5")
        
        # Sauvegarder le scaler et les métadonnées
        model_data = {
            'scaler': self.scaler,
            'sequence_length': self.sequence_length,
            'feature_columns': self.feature_columns,
            'parameters': self.parameters,
            'is_fitted': self.is_fitted
        }
        
        joblib.dump(model_data, f"{filepath}_metadata.pkl")
    
    def load_model(self, filepath: str):
        """Charge le modèle LSTM et le scaler"""
        # Charger le modèle Keras
        self.model = tf.keras.models.load_model(f"{filepath}_model.h5")
        
        # Charger les métadonnées
        model_data = joblib.load(f"{filepath}_metadata.pkl")
        self.scaler = model_data['scaler']
        self.sequence_length = model_data['sequence_length']
        self.feature_columns = model_data['feature_columns']
        self.parameters = model_data['parameters']
        self.is_fitted = model_data['is_fitted']


class GRUModel(BaseModel):
    """Modèle GRU PyTorch robuste pour les données financières séquentielles"""
    
    def __init__(self, name: str = "GRU", parameters: Optional[Dict[str, Any]] = None):
        super().__init__(name, **parameters if parameters else {})
        self.scaler = MinMaxScaler()
        self.sequence_length = self.parameters.get('sequence_length', 5)
        self.hidden_size = self.parameters.get('hidden_size', 16)
        self.num_layers = self.parameters.get('num_layers', 1)
        self.dropout_rate = self.parameters.get('dropout_rate', 0.1)
        self.learning_rate = self.parameters.get('learning_rate', 0.001)
        self.epochs = self.parameters.get('epochs', 5)
        self.batch_size = self.parameters.get('batch_size', 8)
        self.feature_columns = None
        self.is_fitted = False
        self.device = torch.device("cpu")
        
    def _create_model(self):
        if not PYTORCH_AVAILABLE:
            raise ImportError("PyTorch n'est pas disponible. Installez avec: pip install torch")
        
        class RobustGRU(nn.Module):
            def __init__(self, input_size: int, hidden_size: int, output_size: int = 2, num_layers: int = 1, dropout_rate: float = 0.1):
                super(RobustGRU, self).__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                
                # GRU au lieu de LSTM - plus simple et robuste
                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
                self.dropout = nn.Dropout(dropout_rate)
                self.fc = nn.Linear(hidden_size, output_size)
            
            def forward(self, x):
                # Forward pass GRU
                gru_out, _ = self.gru(x)
                # Prendre seulement la dernière sortie
                last_output = gru_out[:, -1, :]
                output = self.fc(self.dropout(last_output))
                return output
        
        input_size = len(self.feature_columns) if self.feature_columns else 1
        return RobustGRU(input_size, self.hidden_size, 2, self.num_layers, self.dropout_rate).to(self.device)
    
    def prepare_sequential_data(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """Prépare les données pour l'entraînement GRU PyTorch"""
        # Convertir en DataFrame si nécessaire
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        if isinstance(y, np.ndarray):
            y = pd.Series(y)
        
        # Normaliser les features
        X_scaled = self.scaler.fit_transform(X)
        
        # Créer les séquences
        X_sequences = []
        y_sequences = []
        
        # S'assurer que nous avons assez de données
        min_length = self.sequence_length + 1
        if len(X_scaled) < min_length:
            raise ValueError(f"Pas assez de données: {len(X_scaled)} échantillons pour des séquences de longueur {self.sequence_length}")
        
        for i in range(self.sequence_length, len(X_scaled)):
            X_sequences.append(X_scaled[i-self.sequence_length:i])
            y_sequences.append(y.iloc[i])
        
        return np.array(X_sequences), np.array(y_sequences)
    
    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray], validation_split: float = 0.2) -> Dict[str, Any]:
        """Entraîne le modèle GRU PyTorch"""
        if not PYTORCH_AVAILABLE:
            raise ImportError("PyTorch n'est pas disponible")
        
        # Imports nécessaires pour l'entraînement
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        from sklearn.metrics import accuracy_score
        
        # Convertir en DataFrame si nécessaire
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        if isinstance(y, np.ndarray):
            y = pd.Series(y)
        
        # Réduire les features pour éviter les problèmes de mémoire
        if X.shape[1] > 50:
            print(f"⚠️ Réduction des features de {X.shape[1]} à 50 pour éviter les problèmes de mémoire")
            X = X.iloc[:, :50] if isinstance(X, pd.DataFrame) else X[:, :50]
        
        # Stocker les colonnes de features AVANT de créer le modèle
        self.feature_columns = X.columns.tolist()
        
        # Créer le modèle AVANT de préparer les données séquentielles
        self.model = self._create_model()
        
        # Préparer les données séquentielles
        X_sequences, y_sequences = self.prepare_sequential_data(X, y)
        
        # Configuration de l'entraînement GRU
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        # Diviser les données
        train_size = int((1 - validation_split) * len(X_sequences))
        X_train, X_val = X_sequences[:train_size], X_sequences[train_size:]
        y_train, y_val = y_sequences[:train_size], y_sequences[train_size:]
        
        # Convertir en tensors PyTorch
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)
        y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
        
        # DataLoader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        
        # Entraînement GRU robuste
        start_time = datetime.now()
        
        print(f"Début de l'entraînement GRU sur {self.device}")
        
        for epoch in range(self.epochs):
            self.model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            print(f"Epoch {epoch+1}/{self.epochs}")
            
            for batch_X, batch_y in train_loader:
                batch_count += 1
                optimizer.zero_grad()
                
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                
                if batch_count % 3 == 0:
                    print(f"Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item():.4f}")
            
            # Validation
            self.model.eval()
            val_loss = 0.0
            val_batches = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = self.model(batch_X)
                    val_loss += criterion(outputs, batch_y).item()
                    val_batches += 1
            
            if val_batches > 0:
                avg_val_loss = val_loss / val_batches
                print(f"Epoch {epoch+1}: Loss={epoch_loss/max(batch_count,1):.4f}, Val_Loss={avg_val_loss:.4f}")
            else:
                print(f"Epoch {epoch+1}: Loss={epoch_loss/max(batch_count,1):.4f}")
            
            # Vérifier le timeout (2 minutes max)
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > 120:  # 2 minutes
                print(f"Timeout atteint après {elapsed:.1f}s, arrêt de l'entraînement")
                break
        
        self.training_time = (datetime.now() - start_time).total_seconds()
        self.is_fitted = True
        
        return {
            "training_time": self.training_time,
            "final_loss": epoch_loss / max(batch_count, 1),
            "final_val_accuracy": 0.0  # Pas calculé pour simplifier
        }
    
    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """Prédictions avec le modèle GRU PyTorch"""
        if not self.is_fitted:
            raise ValueError("Le modèle doit être entraîné avant de faire des prédictions")
        
        # Convertir en DataFrame si nécessaire
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        
        # Normaliser les données
        X_scaled = self.scaler.transform(X)
        
        # Créer les séquences
        X_sequences = []
        for i in range(self.sequence_length, len(X_scaled) + 1):
            X_sequences.append(X_scaled[i-self.sequence_length:i])
        
        X_sequences = np.array(X_sequences)
        X_tensor = torch.tensor(X_sequences, dtype=torch.float32).to(self.device)
        
        # Prédictions
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs.data, 1)
        
        return predicted.cpu().numpy()
    
    def predict_proba(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """Probabilités de prédiction"""
        if not self.is_fitted:
            raise ValueError("Le modèle doit être entraîné avant de faire des prédictions")
        
        # Convertir en DataFrame si nécessaire
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
        
        # Normaliser les données
        X_scaled = self.scaler.transform(X)
        
        # Créer les séquences
        X_sequences = []
        for i in range(self.sequence_length, len(X_scaled) + 1):
            X_sequences.append(X_scaled[i-self.sequence_length:i])
        
        X_sequences = np.array(X_sequences)
        X_tensor = torch.tensor(X_sequences, dtype=torch.float32).to(self.device)
        
        # Probabilités
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(X_tensor)
            probabilities = torch.softmax(outputs, dim=1)
        
        return probabilities.cpu().numpy()
    
    def get_feature_importance(self) -> Dict[str, float]:
        """Pour LSTM PyTorch, on ne peut pas facilement extraire l'importance des features"""
        return {}
    
    def save_model(self, filepath: str):
        """Sauvegarde le modèle LSTM PyTorch"""
        if not self.is_fitted:
            raise ValueError("Le modèle doit être entraîné avant d'être sauvegardé")
        
        # Sauvegarder le modèle PyTorch
        torch.save(self.model.state_dict(), f"{filepath}_model.pth")
        
        # Sauvegarder les métadonnées
        model_data = {
            'scaler': self.scaler,
            'sequence_length': self.sequence_length,
            'feature_columns': self.feature_columns,
            'parameters': self.parameters,
            'is_fitted': self.is_fitted,
            'hidden_sizes': self.hidden_sizes,
            'dropout_rate': self.dropout_rate,
            'learning_rate': self.learning_rate,
            'epochs': self.epochs,
            'batch_size': self.batch_size
        }
        
        joblib.dump(model_data, f"{filepath}_metadata.pkl")
    
    def load_model(self, filepath: str):
        """Charge le modèle LSTM PyTorch"""
        # Charger les métadonnées
        model_data = joblib.load(f"{filepath}_metadata.pkl")
        self.scaler = model_data['scaler']
        self.sequence_length = model_data['sequence_length']
        self.feature_columns = model_data['feature_columns']
        self.parameters = model_data['parameters']
        self.is_fitted = model_data['is_fitted']
        self.hidden_sizes = model_data['hidden_sizes']
        self.dropout_rate = model_data['dropout_rate']
        self.learning_rate = model_data['learning_rate']
        self.epochs = model_data['epochs']
        self.batch_size = model_data['batch_size']
        
        # Créer et charger le modèle
        self.model = self._create_model()
        self.model.load_state_dict(torch.load(f"{filepath}_model.pth", map_location=self.device))
        self.model.eval()


class ModelComparisonFramework:
    """Framework principal pour la comparaison de modèles"""
    
    def __init__(self, results_dir: str = "model_comparison_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        self.models: Dict[str, BaseModel] = {}
        self.results: Dict[str, ModelMetrics] = {}
        self.trading_results: Dict[str, TradingResults] = {}
        
        # Configuration par défaut
        self.default_models = self._get_default_models()
        
        # Charger automatiquement les modèles par défaut
        for name, model in self.default_models.items():
            self.models[name] = model
        
    def _get_default_models(self) -> Dict[str, BaseModel]:
        """Obtenir les modèles par défaut"""
        models = {
            'RandomForest': RandomForestModel('RandomForest'),
            'RandomForest_Tuned': RandomForestModel(
                'RandomForest_Tuned',
                n_estimators=200,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2
            )
        }
        
        # Ajouter XGBoost si disponible
        if XGBOOST_AVAILABLE:
            models['XGBoost'] = XGBoostModel('XGBoost')
            models['XGBoost_Tuned'] = XGBoostModel(
                'XGBoost_Tuned',
                n_estimators=200,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8
            )
        
        # Ajouter LightGBM si disponible
        if LIGHTGBM_AVAILABLE:
            models['LightGBM'] = LightGBMModel('LightGBM')
            models['LightGBM_Tuned'] = LightGBMModel(
                'LightGBM_Tuned',
                n_estimators=200,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8
            )
        
        # Ajouter Neural Network si disponible
        if NEURAL_NETWORKS_AVAILABLE:
            models['NeuralNetwork'] = NeuralNetworkModel('NeuralNetwork')
            models['NeuralNetwork_Tuned'] = NeuralNetworkModel(
                'NeuralNetwork_Tuned',
                hidden_layer_sizes=(200, 100, 50),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=2000
            )
        
        # Ajouter GRU si PyTorch est disponible (modèle robuste recommandé)
        if PYTORCH_AVAILABLE:
            models['GRU'] = GRUModel('GRU')
            models['GRU_Tuned'] = GRUModel(
                'GRU_Tuned',
                parameters={
                    'sequence_length': 7,  # Séquence modérée
                    'hidden_size': 32,  # Taille cachée modérée
                    'num_layers': 2,  # Deux couches pour plus de capacité
                    'dropout_rate': 0.1,  # Dropout léger
                    'learning_rate': 0.001,
                    'epochs': 8,  # Épochs modérés
                    'batch_size': 16  # Batch size modéré
                }
            )
        
        # Ajouter LSTM si TensorFlow est disponible
        if TENSORFLOW_AVAILABLE:
            models['LSTM'] = LSTMModel('LSTM')
            models['LSTM_Tuned'] = LSTMModel(
                'LSTM_Tuned',
                sequence_length=30,  # Lookback plus court pour plus de données
                lstm_units=[64, 32],  # Architecture plus simple
                dropout_rate=0.2,
                learning_rate=0.001,
                epochs=50,
                batch_size=16,
                patience=15
            )
            models['LSTM_Deep'] = LSTMModel(
                'LSTM_Deep',
                sequence_length=60,  # Lookback plus long
                lstm_units=[128, 64, 32],  # Architecture plus profonde
                dropout_rate=0.3,
                learning_rate=0.0005,
                epochs=100,
                batch_size=32,
                patience=25,
                l1_reg=0.01,
                l2_reg=0.01
            )
        
        return models
    
    def add_model(self, name: str, model: BaseModel):
        """Ajouter un modèle personnalisé"""
        self.models[name] = model
        logger.info(f"Modèle {name} ajouté au framework")
    
    def compare_models(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
        prices: Optional[np.ndarray] = None,
        models_to_test: Optional[List[str]] = None
    ) -> Dict[str, ModelMetrics]:
        """
        Comparer les performances de plusieurs modèles
        
        Args:
            X_train: Features d'entraînement
            y_train: Labels d'entraînement
            X_test: Features de test
            y_test: Labels de test
            prices: Prix historiques pour le backtesting (optionnel)
            models_to_test: Liste des modèles à tester (optionnel)
        
        Returns:
            Dictionnaire des métriques pour chaque modèle
        """
        if models_to_test is None:
            models_to_test = list(self.models.keys())
        
        logger.info(f"Début de la comparaison de {len(models_to_test)} modèles")
        
        for model_name in models_to_test:
            try:
                logger.info(f"Entraînement du modèle: {model_name}")
                
                # Obtenir le modèle
                if model_name in self.models:
                    model = self.models[model_name]
                elif model_name in self.default_models:
                    model = self.default_models[model_name]
                else:
                    logger.warning(f"Modèle {model_name} non trouvé, ignoré")
                    continue
                
                # Entraîner le modèle
                if isinstance(model, LSTMModel):
                    # Pour LSTM, on a besoin de données séquentielles
                    # Convertir les arrays numpy en DataFrames pour LSTM
                    X_train_df = pd.DataFrame(X_train)
                    y_train_series = pd.Series(y_train)
                    X_test_df = pd.DataFrame(X_test)
                    
                    # Entraîner avec validation split
                    training_info = model.fit(X_train_df, y_train_series, validation_split=0.2)
                    logger.info(f"Entraînement LSTM terminé - Epochs: {training_info['best_epoch']}, "
                              f"Loss: {training_info['final_loss']:.4f}, "
                              f"Val Loss: {training_info['final_val_loss']:.4f}")
                    
                    # Prédictions
                    y_pred = model.predict(X_test_df)
                    y_pred_proba = model.predict_proba(X_test_df)
                    
                    # Pour LSTM, ajuster les targets car on perd des échantillons à cause de la séquence
                    sequence_length = model.sequence_length
                    y_test_adjusted = y_test[sequence_length:]
                    
                else:
                    # Entraînement standard pour les autres modèles
                    model.fit(X_train, y_train)
                    
                    # Prédictions
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)
                    y_test_adjusted = y_test
                
                # Calculer les métriques ML
                ml_metrics = self._calculate_ml_metrics(y_test_adjusted, y_pred, y_pred_proba)
                
                # Calculer les métriques de trading si les prix sont fournis
                trading_metrics = None
                if prices is not None:
                    # Ajuster les prix aussi pour LSTM
                    if isinstance(model, LSTMModel):
                        prices_adjusted = prices[sequence_length:]
                    else:
                        prices_adjusted = prices
                    
                    trading_metrics = self._calculate_trading_metrics(
                        y_pred, y_pred_proba, prices_adjusted
                    )
                
                # Créer les métriques complètes
                metrics = ModelMetrics(
                    accuracy=convert_numpy_types(ml_metrics['accuracy']),
                    precision=convert_numpy_types(ml_metrics['precision']),
                    recall=convert_numpy_types(ml_metrics['recall']),
                    f1_score=convert_numpy_types(ml_metrics['f1_score']),
                    roc_auc=convert_numpy_types(ml_metrics['roc_auc']),
                    sharpe_ratio=convert_numpy_types(trading_metrics['sharpe_ratio']) if trading_metrics else 0.0,
                    max_drawdown=convert_numpy_types(trading_metrics['max_drawdown']) if trading_metrics else 0.0,
                    total_return=convert_numpy_types(trading_metrics['total_return']) if trading_metrics else 0.0,
                    win_rate=convert_numpy_types(trading_metrics['win_rate']) if trading_metrics else 0.0,
                    profit_factor=convert_numpy_types(trading_metrics['profit_factor']) if trading_metrics else 0.0,
                    training_time=convert_numpy_types(getattr(model, 'training_time', 0.0)),
                    prediction_time=convert_numpy_types(getattr(model, 'prediction_time', 0.0)),
                    model_name=model_name,
                    timestamp=datetime.now(),
                    parameters=model.parameters
                )
                
                self.results[model_name] = metrics
                
                logger.info(f"Modèle {model_name} terminé - Accuracy: {metrics.accuracy:.3f}")
                
            except Exception as e:
                logger.error(f"Erreur lors de l'entraînement de {model_name}: {e}")
                continue
        
        # Sauvegarder les résultats
        self._save_results()
        
        logger.info(f"Comparaison terminée. {len(self.results)} modèles évalués")
        return self.results
    
    def _calculate_ml_metrics(
        self, 
        y_true: np.ndarray, 
        y_pred: np.ndarray, 
        y_pred_proba: np.ndarray
    ) -> Dict[str, float]:
        """Calculer les métriques ML classiques"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted'),
            'recall': recall_score(y_true, y_pred, average='weighted'),
            'f1_score': f1_score(y_true, y_pred, average='weighted'),
            'roc_auc': roc_auc_score(y_true, y_pred_proba[:, 1]) if y_pred_proba.shape[1] > 1 else 0.0
        }
    
    def _calculate_trading_metrics(
        self, 
        y_pred: np.ndarray, 
        y_pred_proba: np.ndarray, 
        prices: np.ndarray
    ) -> Dict[str, float]:
        """Calculer les métriques de trading"""
        # Simulation de trading simple
        trades = []
        position = 0
        entry_price = 0
        equity = [10000]  # Capital initial
        
        for i in range(1, len(prices)):
            current_price = prices[i]
            previous_price = prices[i-1]
            
            # Signal d'achat (prediction = 1)
            if y_pred[i] == 1 and position == 0:
                position = 1
                entry_price = current_price
                trades.append({
                    'type': 'buy',
                    'price': current_price,
                    'date': i,
                    'confidence': y_pred_proba[i, 1] if y_pred_proba.shape[1] > 1 else 0.5
                })
            
            # Signal de vente (prediction = 0) ou stop loss
            elif position == 1 and (y_pred[i] == 0 or current_price <= entry_price * 0.95):
                position = 0
                pnl = (current_price - entry_price) / entry_price
                trades.append({
                    'type': 'sell',
                    'price': current_price,
                    'date': i,
                    'pnl': pnl,
                    'entry_price': entry_price
                })
            
            # Mettre à jour l'equity
            if position == 1:
                current_equity = equity[-1] * (current_price / entry_price)
            else:
                current_equity = equity[-1]
            
            equity.append(current_equity)
        
        # Calculer les métriques
        if len(trades) > 0:
            sell_trades = [t for t in trades if t['type'] == 'sell']
            if sell_trades:
                returns = [t['pnl'] for t in sell_trades]
                winning_trades = [r for r in returns if r > 0]
                losing_trades = [r for r in returns if r < 0]
                
                total_return = (equity[-1] - equity[0]) / equity[0]
                win_rate = len(winning_trades) / len(returns) if returns else 0
                
                # Sharpe ratio (simplifié)
                if len(returns) > 1:
                    sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)
                else:
                    sharpe_ratio = 0
                
                # Max drawdown
                peak = equity[0]
                max_dd = 0
                for value in equity:
                    if value > peak:
                        peak = value
                    dd = (peak - value) / peak
                    if dd > max_dd:
                        max_dd = dd
                
                # Profit factor
                gross_profit = sum(winning_trades) if winning_trades else 0
                gross_loss = abs(sum(losing_trades)) if losing_trades else 0
                profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
                
                return {
                    'sharpe_ratio': sharpe_ratio,
                    'max_drawdown': max_dd,
                    'total_return': total_return,
                    'win_rate': win_rate,
                    'profit_factor': profit_factor
                }
        
        return {
            'sharpe_ratio': 0.0,
            'max_drawdown': 0.0,
            'total_return': 0.0,
            'win_rate': 0.0,
            'profit_factor': 0.0
        }
    
    def get_best_model(self, metric: str = 'f1_score') -> Tuple[str, ModelMetrics]:
        """Obtenir le meilleur modèle selon une métrique"""
        if not self.results:
            raise ValueError("Aucun résultat disponible. Exécutez compare_models() d'abord.")
        
        best_model_name = max(
            self.results.keys(),
            key=lambda x: getattr(self.results[x], metric)
        )
        
        return best_model_name, self.results[best_model_name]
    
    def _save_results(self):
        """Sauvegarder les résultats"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarder les métriques
        results_data = {}
        for model_name, metrics in self.results.items():
            results_data[model_name] = {
                'accuracy': metrics.accuracy,
                'precision': metrics.precision,
                'recall': metrics.recall,
                'f1_score': metrics.f1_score,
                'roc_auc': metrics.roc_auc,
                'sharpe_ratio': metrics.sharpe_ratio,
                'max_drawdown': metrics.max_drawdown,
                'total_return': metrics.total_return,
                'win_rate': metrics.win_rate,
                'profit_factor': metrics.profit_factor,
                'training_time': metrics.training_time,
                'prediction_time': metrics.prediction_time,
                'timestamp': metrics.timestamp.isoformat(),
                'parameters': metrics.parameters
            }
        
        results_file = self.results_dir / f"model_comparison_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)
        
        logger.info(f"Résultats sauvegardés dans {results_file}")
    
    def generate_report(self) -> str:
        """Générer un rapport de comparaison"""
        if not self.results:
            return "Aucun résultat disponible."
        
        report = []
        report.append("=" * 80)
        report.append("RAPPORT DE COMPARAISON DE MODÈLES")
        report.append("=" * 80)
        report.append(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Nombre de modèles comparés: {len(self.results)}")
        report.append("")
        
        # Tableau des résultats
        report.append("RÉSULTATS DÉTAILLÉS:")
        report.append("-" * 80)
        report.append(f"{'Modèle':<20} {'Accuracy':<10} {'F1-Score':<10} {'Sharpe':<10} {'Return':<10}")
        report.append("-" * 80)
        
        for model_name, metrics in self.results.items():
            report.append(
                f"{model_name:<20} "
                f"{metrics.accuracy:<10.3f} "
                f"{metrics.f1_score:<10.3f} "
                f"{metrics.sharpe_ratio:<10.3f} "
                f"{metrics.total_return:<10.3f}"
            )
        
        report.append("-" * 80)
        
        # Meilleur modèle par métrique
        report.append("\nMEILLEURS MODÈLES PAR MÉTRIQUE:")
        report.append("-" * 40)
        
        metrics_to_check = ['accuracy', 'f1_score', 'sharpe_ratio', 'total_return']
        for metric in metrics_to_check:
            try:
                best_name, best_metrics = self.get_best_model(metric)
                report.append(f"{metric.capitalize()}: {best_name} ({getattr(best_metrics, metric):.3f})")
            except:
                continue
        
        return "\n".join(report)

# Fonction utilitaire pour créer le framework
def create_model_comparison_framework() -> ModelComparisonFramework:
    """Créer une instance du framework avec les modèles par défaut"""
    return ModelComparisonFramework()

if __name__ == "__main__":
    # Exemple d'utilisation
    print("Framework de Comparaison de Modèles ML pour le Trading")
    print("=" * 60)
    
    # Créer le framework
    framework = create_model_comparison_framework()
    
    # Afficher les modèles disponibles
    print(f"Modèles disponibles: {list(framework.default_models.keys())}")
    
    # Exemple de données factices pour test
    np.random.seed(42)
    X_train = np.random.randn(1000, 10)
    y_train = np.random.randint(0, 2, 1000)
    X_test = np.random.randn(200, 10)
    y_test = np.random.randint(0, 2, 200)
    prices = np.random.randn(200) * 100 + 100  # Prix simulés
    
    # Comparer les modèles
    print("\nDébut de la comparaison...")
    results = framework.compare_models(X_train, y_train, X_test, y_test, prices)
    
    # Générer le rapport
    report = framework.generate_report()
    print("\n" + report)

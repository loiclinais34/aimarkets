#!/usr/bin/env python3
"""
Script d'analyse des insights ML bas√©s sur les performances des recommandations
Fournit des recommandations concr√®tes pour l'impl√©mentation des outils ML
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MLPerformanceInsights:
    """Analyseur d'insights ML bas√© sur les performances"""
    
    def __init__(self, analysis_file: str = "recommendation_performance_analysis.json"):
        self.analysis_file = os.path.join(os.path.dirname(__file__), analysis_file)
        self.data = self.load_analysis_data()
    
    def load_analysis_data(self) -> Dict:
        """Charge les donn√©es d'analyse"""
        try:
            with open(self.analysis_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"Fichier d'analyse non trouv√©: {self.analysis_file}")
            return {}
    
    def analyze_recommendation_effectiveness(self) -> Dict:
        """Analyse l'efficacit√© des recommandations"""
        logger.info("üéØ Analyse de l'efficacit√© des recommandations")
        
        overall = self.data.get("overall_performance", {})
        accuracy = self.data.get("accuracy_by_recommendation", {})
        
        # Probl√®mes identifi√©s
        problems = []
        recommendations = []
        
        # 1. D√©s√©quilibre des classes
        rec_dist = overall.get("recommendation_distribution", {})
        total = sum(rec_dist.values())
        
        if rec_dist.get("HOLD", 0) / total > 0.9:
            problems.append("D√©s√©quilibre majeur des classes: 95.2% HOLD vs 4.8% BUY/SELL")
            recommendations.append("Impl√©menter un syst√®me de r√©√©chantillonnage (SMOTE) ou ajuster les seuils de d√©cision")
        
        # 2. Faible pr√©cision des signaux actifs
        accuracy_1d = accuracy.get("1d", {})
        buy_accuracy = accuracy_1d.get("BUY", {}).get("accuracy", 0)
        sell_accuracy = accuracy_1d.get("SELL", {}).get("accuracy", 0)
        
        if buy_accuracy < 30 or sell_accuracy < 30:
            problems.append(f"Faible pr√©cision des signaux actifs: BUY {buy_accuracy:.1f}%, SELL {sell_accuracy:.1f}%")
            recommendations.append("Am√©liorer la qualit√© des features et impl√©menter un syst√®me de confiance")
        
        # 3. Retours moyens n√©gatifs pour BUY
        buy_return = accuracy_1d.get("BUY", {}).get("avg_return", 0)
        try:
            buy_return_float = float(buy_return) if buy_return is not None else 0
            if buy_return_float < 0:
                problems.append(f"Retours moyens n√©gatifs pour BUY: {buy_return_float:.2f}%")
                recommendations.append("Revoir la logique de scoring et les seuils de d√©cision")
        except (ValueError, TypeError):
            pass
        
        return {
            "problems": problems,
            "recommendations": recommendations,
            "class_balance": {
                "hold_percentage": (rec_dist.get("HOLD", 0) / total) * 100,
                "active_signals_percentage": ((rec_dist.get("BUY", 0) + rec_dist.get("SELL", 0)) / total) * 100
            },
            "signal_quality": {
                "buy_accuracy": buy_accuracy,
                "sell_accuracy": sell_accuracy,
                "buy_avg_return": buy_return_float if 'buy_return_float' in locals() else 0,
                "sell_avg_return": accuracy_1d.get("SELL", {}).get("avg_return", 0)
            }
        }
    
    def analyze_feature_importance(self) -> Dict:
        """Analyse l'importance des features"""
        logger.info("üìä Analyse de l'importance des features")
        
        correlations = self.data.get("score_correlation", {}).get("correlations", {})
        
        # Analyser les corr√©lations
        feature_importance = {}
        for return_period, corr_data in correlations.items():
            for feature, corr_value in corr_data.items():
                if feature not in feature_importance:
                    feature_importance[feature] = []
                feature_importance[feature].append(abs(corr_value))
        
        # Calculer l'importance moyenne
        avg_importance = {}
        for feature, corr_values in feature_importance.items():
            avg_importance[feature] = np.mean(corr_values)
        
        # Trier par importance
        sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)
        
        # Recommandations
        recommendations = []
        if sorted_features:
            top_feature = sorted_features[0][0]
            recommendations.append(f"Feature principale: {top_feature} (corr√©lation moyenne: {sorted_features[0][1]:.3f})")
            
            if len(sorted_features) > 1:
                recommendations.append(f"Feature secondaire: {sorted_features[1][0]} (corr√©lation moyenne: {sorted_features[1][1]:.3f})")
        
        return {
            "feature_importance": dict(sorted_features),
            "recommendations": recommendations,
            "correlation_details": correlations
        }
    
    def analyze_temporal_patterns(self) -> Dict:
        """Analyse des patterns temporels"""
        logger.info("üìÖ Analyse des patterns temporels")
        
        temporal_data = self.data.get("temporal_performance", {})
        
        # Analyser les variations mensuelles
        monthly_accuracy = {}
        monthly_returns = {}
        
        for month, rec_data in temporal_data.items():
            for rec_type, data in rec_data.items():
                if rec_type not in monthly_accuracy:
                    monthly_accuracy[rec_type] = []
                    monthly_returns[rec_type] = []
                
                try:
                    accuracy_val = float(data.get("accuracy_1d", 0)) if data.get("accuracy_1d") is not None else 0
                    return_val = float(data.get("avg_return_1d", 0)) if data.get("avg_return_1d") is not None else 0
                    monthly_accuracy[rec_type].append(accuracy_val)
                    monthly_returns[rec_type].append(return_val)
                except (ValueError, TypeError):
                    monthly_accuracy[rec_type].append(0)
                    monthly_returns[rec_type].append(0)
        
        # Calculer les variations
        patterns = {}
        for rec_type in monthly_accuracy:
            if monthly_accuracy[rec_type]:
                patterns[rec_type] = {
                    "accuracy_std": np.std(monthly_accuracy[rec_type]),
                    "return_std": np.std(monthly_returns[rec_type]),
                    "accuracy_range": [min(monthly_accuracy[rec_type]), max(monthly_accuracy[rec_type])],
                    "return_range": [min(monthly_returns[rec_type]), max(monthly_returns[rec_type])]
                }
        
        # Recommandations
        recommendations = []
        if patterns:
            recommendations.append("Impl√©menter des features temporelles (mois, saisons, jours de la semaine)")
            recommendations.append("Consid√©rer des mod√®les adaptatifs qui s'ajustent aux conditions de march√©")
        
        return {
            "temporal_patterns": patterns,
            "recommendations": recommendations,
            "monthly_data": temporal_data
        }
    
    def analyze_risk_performance(self) -> Dict:
        """Analyse des performances par niveau de risque"""
        logger.info("‚ö†Ô∏è Analyse des performances par risque")
        
        risk_data = self.data.get("accuracy_by_risk_level", {})
        
        # Analyser les performances par risque
        risk_analysis = {}
        for period, period_data in risk_data.items():
            risk_analysis[period] = {}
            for risk_level, data in period_data.items():
                if data.get("count", 0) > 0:
                    risk_analysis[period][risk_level] = {
                        "accuracy": data.get("accuracy", 0),
                        "avg_return": data.get("avg_return", 0),
                        "volatility": data.get("volatility", 0),
                        "sharpe_ratio": data.get("sharpe_ratio", 0),
                        "count": data.get("count", 0)
                    }
        
        # Recommandations
        recommendations = []
        recommendations.append("Impl√©menter un syst√®me de gestion du risque adaptatif")
        recommendations.append("Utiliser le Sharpe ratio comme m√©trique de r√©compense")
        recommendations.append("Consid√©rer des mod√®les de r√©gularisation bas√©s sur le risque")
        
        return {
            "risk_analysis": risk_analysis,
            "recommendations": recommendations
        }
    
    def generate_ml_architecture_recommendations(self) -> Dict:
        """G√©n√®re des recommandations d'architecture ML"""
        logger.info("üèóÔ∏è G√©n√©ration de recommandations d'architecture ML")
        
        # Architecture recommand√©e
        architecture = {
            "data_preprocessing": [
                "Normalisation des features (StandardScaler)",
                "Gestion du d√©s√©quilibre des classes (SMOTE ou ajustement des seuils)",
                "Feature engineering temporel (mois, saisons, patterns)",
                "Validation crois√©e temporelle (TimeSeriesSplit)"
            ],
            "model_architecture": [
                "Mod√®le principal: Random Forest ou XGBoost pour la robustesse",
                "Mod√®le secondaire: Neural Network pour capturer les interactions complexes",
                "Ensemble method: Voting ou Stacking des mod√®les",
                "Mod√®le de confiance: S√©par√© pour estimer la fiabilit√© des pr√©dictions"
            ],
            "feature_engineering": [
                "Scores techniques comme features principales",
                "Interactions entre scores (technique √ó sentiment)",
                "Features temporelles (tendances, saisonnalit√©)",
                "Features de volatilit√© et de risque",
                "Features de march√© (corr√©lations, r√©gimes)"
            ],
            "training_strategy": [
                "Validation crois√©e temporelle pour √©viter le data leakage",
                "M√©triques de r√©compense: Sharpe ratio + accuracy",
                "R√©gularisation pour √©viter le surapprentissage",
                "Early stopping bas√© sur la performance de validation"
            ],
            "deployment_strategy": [
                "Syst√®me de confiance pour filtrer les pr√©dictions peu fiables",
                "Monitoring en temps r√©el des performances",
                "Mise √† jour p√©riodique des mod√®les",
                "A/B testing pour valider les am√©liorations"
            ]
        }
        
        return architecture
    
    def generate_actionable_insights(self) -> Dict:
        """G√©n√®re des insights actionnables"""
        logger.info("üí° G√©n√©ration d'insights actionnables")
        
        # Insights prioritaires
        priority_insights = [
            {
                "priority": "HIGH",
                "title": "Corriger le d√©s√©quilibre des classes",
                "description": "95.2% des recommandations sont HOLD, ce qui limite l'utilit√© du mod√®le",
                "action": "Ajuster les seuils de d√©cision ou impl√©menter SMOTE",
                "impact": "Am√©lioration significative de la diversit√© des signaux"
            },
            {
                "priority": "HIGH", 
                "title": "Am√©liorer la pr√©cision des signaux actifs",
                "description": "BUY et SELL ont une pr√©cision de ~26%, insuffisante pour le trading",
                "action": "Am√©liorer la qualit√© des features et impl√©menter un syst√®me de confiance",
                "impact": "R√©duction des faux signaux et am√©lioration des performances"
            },
            {
                "priority": "MEDIUM",
                "title": "Impl√©menter des features temporelles",
                "description": "Les performances varient dans le temps, indiquant des patterns saisonniers",
                "action": "Ajouter des features de temps (mois, saisons, jours de la semaine)",
                "impact": "Am√©lioration de la robustesse temporelle"
            },
            {
                "priority": "MEDIUM",
                "title": "Optimiser la gestion du risque",
                "description": "Les performances varient selon le niveau de risque",
                "action": "Impl√©menter un syst√®me de gestion du risque adaptatif",
                "impact": "Am√©lioration du ratio risque/rendement"
            }
        ]
        
        return {
            "priority_insights": priority_insights,
            "next_steps": [
                "1. Impl√©menter un syst√®me de r√©√©chantillonnage pour √©quilibrer les classes",
                "2. D√©velopper un mod√®le de confiance pour filtrer les pr√©dictions peu fiables",
                "3. Cr√©er des features temporelles et de march√© avanc√©es",
                "4. Mettre en place un syst√®me de validation crois√©e temporelle",
                "5. D√©velopper un pipeline de d√©ploiement avec monitoring"
            ]
        }
    
    def run_complete_analysis(self) -> Dict:
        """Ex√©cute l'analyse compl√®te des insights ML"""
        logger.info("üöÄ D√©marrage de l'analyse compl√®te des insights ML")
        
        try:
            results = {
                "recommendation_effectiveness": self.analyze_recommendation_effectiveness(),
                "feature_importance": self.analyze_feature_importance(),
                "temporal_patterns": self.analyze_temporal_patterns(),
                "risk_performance": self.analyze_risk_performance(),
                "ml_architecture": self.generate_ml_architecture_recommendations(),
                "actionable_insights": self.generate_actionable_insights()
            }
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
            raise
    
    def print_summary(self):
        """Affiche un r√©sum√© des insights ML"""
        if not self.data:
            logger.warning("Aucune donn√©e d'analyse disponible")
            return
        
        results = self.run_complete_analysis()
        
        print("\n" + "="*80)
        print("ü§ñ INSIGHTS ML POUR L'AM√âLIORATION DES PERFORMANCES")
        print("="*80)
        
        # Probl√®mes critiques
        effectiveness = results["recommendation_effectiveness"]
        print(f"\nüö® PROBL√àMES CRITIQUES IDENTIFI√âS:")
        for i, problem in enumerate(effectiveness["problems"], 1):
            print(f"  {i}. {problem}")
        
        # Recommandations prioritaires
        insights = results["actionable_insights"]
        print(f"\nüéØ RECOMMANDATIONS PRIORITAIRES:")
        for insight in insights["priority_insights"]:
            print(f"  [{insight['priority']}] {insight['title']}")
            print(f"      {insight['description']}")
            print(f"      Action: {insight['action']}")
            print(f"      Impact: {insight['impact']}\n")
        
        # Architecture ML recommand√©e
        architecture = results["ml_architecture"]
        print(f"üèóÔ∏è ARCHITECTURE ML RECOMMAND√âE:")
        print(f"  ‚Ä¢ Mod√®le principal: Random Forest ou XGBoost")
        print(f"  ‚Ä¢ Mod√®le secondaire: Neural Network")
        print(f"  ‚Ä¢ Ensemble method: Voting ou Stacking")
        print(f"  ‚Ä¢ Syst√®me de confiance: S√©par√© pour la fiabilit√©")
        
        # Prochaines √©tapes
        print(f"\nüìã PROCHAINES √âTAPES:")
        for step in insights["next_steps"]:
            print(f"  {step}")
        
        print("\n" + "="*80)


def main():
    """Fonction principale"""
    
    logger.info("üöÄ D√©marrage de l'analyse des insights ML")
    
    try:
        # Cr√©er l'analyseur
        analyzer = MLPerformanceInsights()
        
        # Ex√©cuter l'analyse
        results = analyzer.run_complete_analysis()
        
        # Afficher le r√©sum√©
        analyzer.print_summary()
        
        # Sauvegarder les r√©sultats
        output_file = os.path.join(os.path.dirname(__file__), "ml_performance_insights.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üìÅ Insights sauvegard√©s dans {output_file}")
        logger.info("‚úÖ Analyse des insights ML termin√©e avec succ√®s")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

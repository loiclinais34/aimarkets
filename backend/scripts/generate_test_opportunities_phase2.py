#!/usr/bin/env python3
"""
Script pour g√©n√©rer des opportunit√©s historiques de test avec les am√©liorations Phase 1 et 2
G√©n√®re des opportunit√©s pour 20 titres depuis mai 2025
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import pandas as pd
import numpy as np
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.services.ml_backtesting.historical_opportunity_generator import HistoricalOpportunityGenerator
from app.services.ml_backtesting.opportunity_validator import OpportunityValidator
from app.models.database import HistoricalData
from app.models.historical_opportunities import HistoricalOpportunities, HistoricalOpportunityValidation
import logging
from typing import Dict, List
import json
from datetime import datetime, date, timedelta

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Phase2TestGenerator:
    """G√©n√©rateur de test pour les am√©liorations Phase 1 et 2"""
    
    def __init__(self, db: Session):
        self.db = db
        self.generator = HistoricalOpportunityGenerator(db)
        self.validator = OpportunityValidator(db)
        self.results = {}
    
    def get_test_symbols(self) -> List[str]:
        """Retourne la liste des 20 symboles de test"""
        # Symboles repr√©sentatifs de diff√©rentes capitalisations et secteurs
        return [
            # Tech Giants
            "AAPL", "MSFT", "GOOGL", "AMZN", "META",
            # Semiconductor
            "NVDA", "AMD", "INTC", "TSM", "AVGO",
            # Finance
            "JPM", "BAC", "WFC", "GS", "MS",
            # Healthcare
            "JNJ", "PFE", "UNH", "ABBV", "MRK"
        ]
    
    def get_date_range(self) -> tuple:
        """Retourne la plage de dates pour le test (mai 2025)"""
        start_date = date(2025, 5, 1)
        end_date = date(2025, 5, 31)
        return start_date, end_date
    
    async def generate_opportunities_for_symbols(self) -> Dict:
        """G√©n√®re des opportunit√©s pour tous les symboles de test"""
        logger.info("üöÄ D√©marrage de la g√©n√©ration d'opportunit√©s de test")
        
        symbols = self.get_test_symbols()
        start_date, end_date = self.get_date_range()
        
        logger.info(f"üìä Configuration du test:")
        logger.info(f"  ‚Ä¢ Symboles: {len(symbols)}")
        logger.info(f"  ‚Ä¢ P√©riode: {start_date} √† {end_date}")
        logger.info(f"  ‚Ä¢ Jours ouvr√©s estim√©s: ~22")
        logger.info(f"  ‚Ä¢ Opportunit√©s estim√©es: ~{len(symbols) * 22}")
        
        results = {
            "symbols_processed": [],
            "total_opportunities": 0,
            "errors": [],
            "start_time": datetime.now().isoformat()
        }
        
        for symbol in symbols:
            try:
                logger.info(f"üìà Traitement de {symbol}...")
                
                # V√©rifier que des donn√©es historiques existent pour ce symbole
                has_data = self.db.query(HistoricalData).filter(
                    HistoricalData.symbol == symbol,
                    HistoricalData.date >= start_date,
                    HistoricalData.date <= end_date
                ).first() is not None
                
                if not has_data:
                    logger.warning(f"‚ö†Ô∏è  Aucune donn√©e historique pour {symbol} sur la p√©riode")
                    results["errors"].append(f"{symbol}: Pas de donn√©es historiques")
                    continue
                
                # G√©n√©rer les opportunit√©s pour ce symbole
                symbol_opportunities = 0
                current_date = start_date
                
                while current_date <= end_date:
                    try:
                        # G√©n√©rer une opportunit√© pour cette date
                        opportunity = await self.generator._generate_opportunity_for_symbol_date(
                            symbol=symbol,
                            target_date=current_date
                        )
                        
                        if opportunity:
                            # Sauvegarder l'opportunit√©
                            self.db.add(opportunity)
                            self.db.commit()
                            symbol_opportunities += 1
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Erreur pour {symbol} le {current_date}: {e}")
                        results["errors"].append(f"{symbol} {current_date}: {str(e)}")
                        # Rollback en cas d'erreur
                        self.db.rollback()
                    
                    # Passer au jour suivant
                    current_date += timedelta(days=1)
                
                results["symbols_processed"].append({
                    "symbol": symbol,
                    "opportunities_generated": symbol_opportunities
                })
                
                results["total_opportunities"] += symbol_opportunities
                
                logger.info(f"‚úÖ {symbol}: {symbol_opportunities} opportunit√©s g√©n√©r√©es")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement de {symbol}: {e}")
                results["errors"].append(f"{symbol}: {str(e)}")
        
        results["end_time"] = datetime.now().isoformat()
        results["duration"] = (
            datetime.fromisoformat(results["end_time"]) - 
            datetime.fromisoformat(results["start_time"])
        ).total_seconds()
        
        return results
    
    async def validate_opportunities(self) -> Dict:
        """Valide les performances des opportunit√©s g√©n√©r√©es"""
        logger.info("üîç D√©marrage de la validation des opportunit√©s")
        
        try:
            # R√©cup√©rer toutes les opportunit√©s g√©n√©r√©es
            opportunities = self.db.query(HistoricalOpportunities).all()
            
            # Valider toutes les opportunit√©s
            validation_results = await self.validator.validate_opportunities_batch(opportunities)
            
            return {
                "validation_successful": True,
                "validation_results": validation_results,
                "validation_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la validation: {e}")
            return {
                "validation_successful": False,
                "error": str(e),
                "validation_time": datetime.now().isoformat()
            }
    
    def analyze_results(self) -> Dict:
        """Analyse les r√©sultats de la g√©n√©ration et validation"""
        logger.info("üìä Analyse des r√©sultats")
        
        try:
            # Compter les opportunit√©s g√©n√©r√©es
            total_opportunities = self.db.query(HistoricalOpportunities).count()
            
            # Compter les validations
            total_validations = self.db.query(HistoricalOpportunityValidation).count()
            
            # Analyser les recommandations
            recommendations = self.db.query(HistoricalOpportunities.recommendation).all()
            recommendation_counts = {}
            for rec in recommendations:
                rec_type = rec[0] if rec[0] else "UNKNOWN"
                recommendation_counts[rec_type] = recommendation_counts.get(rec_type, 0) + 1
            
            # Analyser les niveaux de risque
            risk_levels = self.db.query(HistoricalOpportunities.risk_level).all()
            risk_counts = {}
            for risk in risk_levels:
                risk_type = risk[0] if risk[0] else "UNKNOWN"
                risk_counts[risk_type] = risk_counts.get(risk_type, 0) + 1
            
            # Analyser les scores composites
            composite_scores = self.db.query(HistoricalOpportunities.composite_score).all()
            scores = [float(score[0]) for score in composite_scores if score[0] is not None]
            
            score_stats = {
                "count": len(scores),
                "mean": np.mean(scores) if scores else 0,
                "std": np.std(scores) if scores else 0,
                "min": np.min(scores) if scores else 0,
                "max": np.max(scores) if scores else 0,
                "median": np.median(scores) if scores else 0
            }
            
            # Analyser les niveaux de confiance
            confidence_levels = self.db.query(HistoricalOpportunities.confidence_level).all()
            confidences = [float(conf[0]) for conf in confidence_levels if conf[0] is not None]
            
            confidence_stats = {
                "count": len(confidences),
                "mean": np.mean(confidences) if confidences else 0,
                "std": np.std(confidences) if confidences else 0,
                "min": np.min(confidences) if confidences else 0,
                "max": np.max(confidences) if confidences else 0,
                "median": np.median(confidences) if confidences else 0
            }
            
            return {
                "total_opportunities": total_opportunities,
                "total_validations": total_validations,
                "recommendation_distribution": recommendation_counts,
                "risk_level_distribution": risk_counts,
                "composite_score_stats": score_stats,
                "confidence_level_stats": confidence_stats,
                "analysis_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
            return {
                "error": str(e),
                "analysis_time": datetime.now().isoformat()
            }
    
    async def run_complete_test(self) -> Dict:
        """Ex√©cute le test complet"""
        logger.info("üöÄ D√©marrage du test complet Phase 2")
        
        try:
            # G√©n√©rer les opportunit√©s
            generation_results = await self.generate_opportunities_for_symbols()
            
            # Valider les opportunit√©s
            validation_results = await self.validate_opportunities()
            
            # Analyser les r√©sultats
            analysis_results = self.analyze_results()
            
            # Consolider les r√©sultats
            complete_results = {
                "generation": generation_results,
                "validation": validation_results,
                "analysis": analysis_results,
                "test_completed": True,
                "test_timestamp": datetime.now().isoformat()
            }
            
            return complete_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du test complet: {e}")
            return {
                "test_completed": False,
                "error": str(e),
                "test_timestamp": datetime.now().isoformat()
            }
    
    def print_summary(self, results: Dict):
        """Affiche un r√©sum√© des r√©sultats"""
        if not results.get("test_completed", False):
            logger.error("‚ùå Test non termin√© avec succ√®s")
            return
        
        generation = results["generation"]
        validation = results["validation"]
        analysis = results["analysis"]
        
        print("\n" + "="*80)
        print("üß™ R√âSULTATS DU TEST PHASE 2 - G√âN√âRATION D'OPPORTUNIT√âS")
        print("="*80)
        
        print(f"\nüìä G√âN√âRATION:")
        print(f"  ‚Ä¢ Symboles trait√©s: {len(generation['symbols_processed'])}")
        print(f"  ‚Ä¢ Opportunit√©s g√©n√©r√©es: {generation['total_opportunities']}")
        print(f"  ‚Ä¢ Dur√©e: {generation['duration']:.2f} secondes")
        print(f"  ‚Ä¢ Erreurs: {len(generation['errors'])}")
        
        if generation['errors']:
            print("  ‚Ä¢ D√©tail des erreurs:")
            for error in generation['errors'][:5]:  # Afficher les 5 premi√®res erreurs
                print(f"    - {error}")
        
        print(f"\nüîç VALIDATION:")
        print(f"  ‚Ä¢ Validation r√©ussie: {'‚úÖ' if validation['validation_successful'] else '‚ùå'}")
        if not validation['validation_successful']:
            print(f"  ‚Ä¢ Erreur: {validation.get('error', 'Inconnue')}")
        
        print(f"\nüìà ANALYSE:")
        print(f"  ‚Ä¢ Opportunit√©s totales: {analysis['total_opportunities']}")
        print(f"  ‚Ä¢ Validations totales: {analysis['total_validations']}")
        
        print(f"\nüéØ DISTRIBUTION DES RECOMMANDATIONS:")
        for rec_type, count in analysis['recommendation_distribution'].items():
            percentage = (count / analysis['total_opportunities']) * 100 if analysis['total_opportunities'] > 0 else 0
            print(f"  ‚Ä¢ {rec_type}: {count} ({percentage:.1f}%)")
        
        print(f"\n‚ö†Ô∏è  DISTRIBUTION DES NIVEAUX DE RISQUE:")
        for risk_type, count in analysis['risk_level_distribution'].items():
            percentage = (count / analysis['total_opportunities']) * 100 if analysis['total_opportunities'] > 0 else 0
            print(f"  ‚Ä¢ {risk_type}: {count} ({percentage:.1f}%)")
        
        print(f"\nüìä STATISTIQUES DES SCORES COMPOSITES:")
        stats = analysis['composite_score_stats']
        print(f"  ‚Ä¢ Moyenne: {stats['mean']:.3f}")
        print(f"  ‚Ä¢ M√©diane: {stats['median']:.3f}")
        print(f"  ‚Ä¢ √âcart-type: {stats['std']:.3f}")
        print(f"  ‚Ä¢ Min: {stats['min']:.3f}")
        print(f"  ‚Ä¢ Max: {stats['max']:.3f}")
        
        print(f"\nüéØ STATISTIQUES DES NIVEAUX DE CONFIANCE:")
        conf_stats = analysis['confidence_level_stats']
        print(f"  ‚Ä¢ Moyenne: {conf_stats['mean']:.3f}")
        print(f"  ‚Ä¢ M√©diane: {conf_stats['median']:.3f}")
        print(f"  ‚Ä¢ √âcart-type: {conf_stats['std']:.3f}")
        print(f"  ‚Ä¢ Min: {conf_stats['min']:.3f}")
        print(f"  ‚Ä¢ Max: {conf_stats['max']:.3f}")
        
        print("\n" + "="*80)


def main():
    """Fonction principale"""
    
    logger.info("üöÄ D√©marrage du test de g√©n√©ration d'opportunit√©s Phase 2")
    
    db = next(get_db())
    
    try:
        # Cr√©er le g√©n√©rateur de test
        test_generator = Phase2TestGenerator(db)
        
        # Ex√©cuter le test complet
        results = asyncio.run(test_generator.run_complete_test())
        
        # Afficher le r√©sum√©
        test_generator.print_summary(results)
        
        # Sauvegarder les r√©sultats
        output_file = os.path.join(os.path.dirname(__file__), "phase2_test_generation_results.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üìÅ R√©sultats de test sauvegard√©s dans {output_file}")
        logger.info("‚úÖ Test de g√©n√©ration Phase 2 termin√© avec succ√®s")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution du test: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        db.close()


if __name__ == "__main__":
    main()

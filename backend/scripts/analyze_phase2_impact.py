#!/usr/bin/env python3
"""
Script pour analyser l'impact des modifications Phase 1 et 2 sur les performances
Compare les nouvelles opportunitÃ©s avec les anciennes pour mesurer l'amÃ©lioration
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models.historical_opportunities import HistoricalOpportunities, HistoricalOpportunityValidation
import logging
from typing import Dict, List
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Phase2ImpactAnalyzer:
    """Analyseur d'impact des modifications Phase 1 et 2"""
    
    def __init__(self, db: Session):
        self.db = db
        self.results = {}
    
    def analyze_opportunity_distribution(self) -> Dict:
        """Analyse la distribution des opportunitÃ©s gÃ©nÃ©rÃ©es"""
        logger.info("ğŸ“Š Analyse de la distribution des opportunitÃ©s")
        
        # RÃ©cupÃ©rer toutes les opportunitÃ©s
        opportunities = self.db.query(HistoricalOpportunities).all()
        
        if not opportunities:
            return {"error": "Aucune opportunitÃ© trouvÃ©e"}
        
        # Analyser les recommandations
        recommendations = [op.recommendation for op in opportunities if op.recommendation]
        rec_counts = pd.Series(recommendations).value_counts()
        
        # Analyser les niveaux de risque
        risk_levels = [op.risk_level for op in opportunities if op.risk_level]
        risk_counts = pd.Series(risk_levels).value_counts()
        
        # Analyser les scores composites
        composite_scores = [float(op.composite_score) for op in opportunities if op.composite_score is not None]
        
        # Analyser les niveaux de confiance
        confidence_levels = [float(op.confidence_level) for op in opportunities if op.confidence_level is not None]
        
        return {
            "total_opportunities": len(opportunities),
            "recommendation_distribution": rec_counts.to_dict(),
            "risk_level_distribution": risk_counts.to_dict(),
            "composite_score_stats": {
                "mean": np.mean(composite_scores),
                "median": np.median(composite_scores),
                "std": np.std(composite_scores),
                "min": np.min(composite_scores),
                "max": np.max(composite_scores)
            },
            "confidence_level_stats": {
                "mean": np.mean(confidence_levels),
                "median": np.median(confidence_levels),
                "std": np.std(confidence_levels),
                "min": np.min(confidence_levels),
                "max": np.max(confidence_levels)
            }
        }
    
    def analyze_validation_performance(self) -> Dict:
        """Analyse les performances de validation"""
        logger.info("ğŸ” Analyse des performances de validation")
        
        # RÃ©cupÃ©rer toutes les opportunitÃ©s avec leurs donnÃ©es de validation
        opportunities = self.db.query(HistoricalOpportunities).all()
        
        if not opportunities:
            return {"error": "Aucune opportunitÃ© trouvÃ©e"}
        
        # Analyser les performances par pÃ©riode
        periods = [1, 7, 30]
        period_analysis = {}
        
        for period in periods:
            # Filtrer les opportunitÃ©s qui ont des donnÃ©es pour cette pÃ©riode
            if period == 1:
                period_opportunities = [o for o in opportunities if o.return_1_day is not None]
                returns = [float(o.return_1_day) for o in period_opportunities]
                correct_recommendations = [o.recommendation_correct_1_day for o in period_opportunities if o.recommendation_correct_1_day is not None]
            elif period == 7:
                period_opportunities = [o for o in opportunities if o.return_7_days is not None]
                returns = [float(o.return_7_days) for o in period_opportunities]
                correct_recommendations = [o.recommendation_correct_7_days for o in period_opportunities if o.recommendation_correct_7_days is not None]
            elif period == 30:
                period_opportunities = [o for o in opportunities if o.return_30_days is not None]
                returns = [float(o.return_30_days) for o in period_opportunities]
                correct_recommendations = [o.recommendation_correct_30_days for o in period_opportunities if o.recommendation_correct_30_days is not None]
            
            if not period_opportunities:
                continue
            
            period_analysis[period] = {
                "total_opportunities": len(period_opportunities),
                "accuracy": np.mean(correct_recommendations) if correct_recommendations else 0,
                "avg_return": np.mean(returns) if returns else 0,
                "std_return": np.std(returns) if returns else 0,
                "min_return": np.min(returns) if returns else 0,
                "max_return": np.max(returns) if returns else 0,
                "positive_returns": sum(1 for r in returns if r > 0) if returns else 0,
                "negative_returns": sum(1 for r in returns if r < 0) if returns else 0
            }
        
        return period_analysis
    
    def analyze_recommendation_accuracy(self) -> Dict:
        """Analyse la prÃ©cision des recommandations par type"""
        logger.info("ğŸ¯ Analyse de la prÃ©cision des recommandations")
        
        # RÃ©cupÃ©rer les opportunitÃ©s avec leurs validations
        opportunities = self.db.query(HistoricalOpportunities).all()
        
        if not opportunities:
            return {"error": "Aucune opportunitÃ© trouvÃ©e"}
        
        # Grouper par type de recommandation
        recommendation_analysis = {}
        
        for op in opportunities:
            if not op.recommendation:
                continue
            
            rec_type = op.recommendation
            if rec_type not in recommendation_analysis:
                recommendation_analysis[rec_type] = {
                    "count": 0,
                    "correct_1d": [],
                    "correct_7d": [],
                    "correct_30d": [],
                    "returns_1d": [],
                    "returns_7d": [],
                    "returns_30d": []
                }
            
            recommendation_analysis[rec_type]["count"] += 1
            
            # Utiliser directement les donnÃ©es de l'opportunitÃ©
            if op.recommendation_correct_1_day is not None:
                recommendation_analysis[rec_type]["correct_1d"].append(op.recommendation_correct_1_day)
            if op.recommendation_correct_7_days is not None:
                recommendation_analysis[rec_type]["correct_7d"].append(op.recommendation_correct_7_days)
            if op.recommendation_correct_30_days is not None:
                recommendation_analysis[rec_type]["correct_30d"].append(op.recommendation_correct_30_days)
            
            if op.return_1_day is not None:
                recommendation_analysis[rec_type]["returns_1d"].append(float(op.return_1_day))
            if op.return_7_days is not None:
                recommendation_analysis[rec_type]["returns_7d"].append(float(op.return_7_days))
            if op.return_30_days is not None:
                recommendation_analysis[rec_type]["returns_30d"].append(float(op.return_30_days))
        
        # Calculer les statistiques
        for rec_type, data in recommendation_analysis.items():
            data["accuracy_1d"] = np.mean(data["correct_1d"]) if data["correct_1d"] else 0
            data["accuracy_7d"] = np.mean(data["correct_7d"]) if data["correct_7d"] else 0
            data["accuracy_30d"] = np.mean(data["correct_30d"]) if data["correct_30d"] else 0
            data["avg_return_1d"] = np.mean(data["returns_1d"]) if data["returns_1d"] else 0
            data["avg_return_7d"] = np.mean(data["returns_7d"]) if data["returns_7d"] else 0
            data["avg_return_30d"] = np.mean(data["returns_30d"]) if data["returns_30d"] else 0
            data["std_return_1d"] = np.std(data["returns_1d"]) if data["returns_1d"] else 0
            data["std_return_7d"] = np.std(data["returns_7d"]) if data["returns_7d"] else 0
            data["std_return_30d"] = np.std(data["returns_30d"]) if data["returns_30d"] else 0
        
        return recommendation_analysis
    
    def analyze_confidence_impact(self) -> Dict:
        """Analyse l'impact des niveaux de confiance sur les performances"""
        logger.info("ğŸ¯ Analyse de l'impact de la confiance")
        
        # RÃ©cupÃ©rer les opportunitÃ©s avec leurs validations
        opportunities = self.db.query(HistoricalOpportunities).all()
        
        if not opportunities:
            return {"error": "Aucune opportunitÃ© trouvÃ©e"}
        
        # Grouper par niveau de confiance
        confidence_analysis = {}
        
        for op in opportunities:
            if not op.confidence_level:
                continue
            
            conf_level = op.confidence_level
            if conf_level not in confidence_analysis:
                confidence_analysis[conf_level] = {
                    "count": 0,
                    "correct_1d": [],
                    "correct_7d": [],
                    "correct_30d": [],
                    "returns_1d": [],
                    "returns_7d": [],
                    "returns_30d": []
                }
            
            confidence_analysis[conf_level]["count"] += 1
            
            # Utiliser directement les donnÃ©es de l'opportunitÃ©
            if op.recommendation_correct_1_day is not None:
                confidence_analysis[conf_level]["correct_1d"].append(op.recommendation_correct_1_day)
            if op.recommendation_correct_7_days is not None:
                confidence_analysis[conf_level]["correct_7d"].append(op.recommendation_correct_7_days)
            if op.recommendation_correct_30_days is not None:
                confidence_analysis[conf_level]["correct_30d"].append(op.recommendation_correct_30_days)
            
            if op.return_1_day is not None:
                confidence_analysis[conf_level]["returns_1d"].append(float(op.return_1_day))
            if op.return_7_days is not None:
                confidence_analysis[conf_level]["returns_7d"].append(float(op.return_7_days))
            if op.return_30_days is not None:
                confidence_analysis[conf_level]["returns_30d"].append(float(op.return_30_days))
        
        # Calculer les statistiques
        for conf_level, data in confidence_analysis.items():
            data["accuracy_1d"] = np.mean(data["correct_1d"]) if data["correct_1d"] else 0
            data["accuracy_7d"] = np.mean(data["correct_7d"]) if data["correct_7d"] else 0
            data["accuracy_30d"] = np.mean(data["correct_30d"]) if data["correct_30d"] else 0
            data["avg_return_1d"] = np.mean(data["returns_1d"]) if data["returns_1d"] else 0
            data["avg_return_7d"] = np.mean(data["returns_7d"]) if data["returns_7d"] else 0
            data["avg_return_30d"] = np.mean(data["returns_30d"]) if data["returns_30d"] else 0
            data["std_return_1d"] = np.std(data["returns_1d"]) if data["returns_1d"] else 0
            data["std_return_7d"] = np.std(data["returns_7d"]) if data["returns_7d"] else 0
            data["std_return_30d"] = np.std(data["returns_30d"]) if data["returns_30d"] else 0
        
        return confidence_analysis
    
    def generate_insights(self) -> Dict:
        """GÃ©nÃ¨re des insights sur l'impact des modifications"""
        logger.info("ğŸ’¡ GÃ©nÃ©ration d'insights")
        
        insights = {
            "phase_2_improvements": [],
            "key_findings": [],
            "recommendations": []
        }
        
        # Analyser la distribution des opportunitÃ©s
        distribution = self.analyze_opportunity_distribution()
        if "error" not in distribution:
            # Insight sur la distribution des recommandations
            rec_dist = distribution["recommendation_distribution"]
            total = sum(rec_dist.values())
            
            if "BUY_WEAK" in rec_dist and "BUY_MODERATE" in rec_dist:
                buy_opportunities = rec_dist.get("BUY_WEAK", 0) + rec_dist.get("BUY_MODERATE", 0)
                buy_percentage = (buy_opportunities / total) * 100
                insights["key_findings"].append(f"Phase 2 gÃ©nÃ¨re {buy_percentage:.1f}% d'opportunitÃ©s d'achat (BUY_WEAK + BUY_MODERATE)")
            
            # Insight sur les niveaux de confiance
            conf_stats = distribution["confidence_level_stats"]
            if conf_stats["mean"] > 0.8:
                insights["key_findings"].append(f"Niveau de confiance moyen Ã©levÃ©: {conf_stats['mean']:.3f}")
        
        # Analyser les performances de validation
        validation_perf = self.analyze_validation_performance()
        if "error" not in validation_perf:
            for period, data in validation_perf.items():
                if data["accuracy"] > 0:
                    insights["key_findings"].append(f"PrÃ©cision {period}j: {data['accuracy']:.1%}")
        
        # Analyser la prÃ©cision des recommandations
        rec_accuracy = self.analyze_recommendation_accuracy()
        if "error" not in rec_accuracy:
            for rec_type, data in rec_accuracy.items():
                if data.get("accuracy_1d", 0) > 0:
                    insights["key_findings"].append(f"PrÃ©cision {rec_type}: {data['accuracy_1d']:.1%}")
        
        # Recommandations
        insights["recommendations"].extend([
            "Continuer Ã  affiner les seuils de validation pour amÃ©liorer la prÃ©cision",
            "ImplÃ©menter la Phase 3 (gestion du risque) pour optimiser les performances",
            "Surveiller les performances des signaux BUY_WEAK et BUY_MODERATE",
            "Analyser l'impact des niveaux de confiance sur les dÃ©cisions de trading"
        ])
        
        return insights
    
    def run_complete_analysis(self) -> Dict:
        """ExÃ©cute l'analyse complÃ¨te de l'impact Phase 2"""
        logger.info("ğŸš€ DÃ©marrage de l'analyse complÃ¨te de l'impact Phase 2")
        
        try:
            self.results = {
                "opportunity_distribution": self.analyze_opportunity_distribution(),
                "validation_performance": self.analyze_validation_performance(),
                "recommendation_accuracy": self.analyze_recommendation_accuracy(),
                "confidence_impact": self.analyze_confidence_impact(),
                "insights": self.generate_insights(),
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            return self.results
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'analyse: {e}")
            return {
                "error": str(e),
                "analysis_timestamp": datetime.now().isoformat()
            }
    
    def print_summary(self, results: Dict):
        """Affiche un rÃ©sumÃ© de l'analyse"""
        if "error" in results:
            logger.error(f"âŒ Erreur dans l'analyse: {results['error']}")
            return
        
        distribution = results["opportunity_distribution"]
        validation = results["validation_performance"]
        rec_accuracy = results["recommendation_accuracy"]
        confidence = results["confidence_impact"]
        insights = results["insights"]
        
        print("\n" + "="*80)
        print("ğŸ“Š ANALYSE DE L'IMPACT PHASE 2 - AMÃ‰LIORATION DU SCORING")
        print("="*80)
        
        print(f"\nğŸ“ˆ DISTRIBUTION DES OPPORTUNITÃ‰S:")
        if "error" not in distribution:
            print(f"  â€¢ Total: {distribution['total_opportunities']}")
            print(f"  â€¢ Recommandations:")
            for rec, count in distribution["recommendation_distribution"].items():
                percentage = (count / distribution["total_opportunities"]) * 100
                print(f"    - {rec}: {count} ({percentage:.1f}%)")
            print(f"  â€¢ Score composite moyen: {distribution['composite_score_stats']['mean']:.3f}")
            print(f"  â€¢ Confiance moyenne: {distribution['confidence_level_stats']['mean']:.3f}")
        
        print(f"\nğŸ” PERFORMANCES DE VALIDATION:")
        if "error" not in validation:
            for period, data in validation.items():
                print(f"  â€¢ PÃ©riode {period} jours:")
                print(f"    - PrÃ©cision: {data['accuracy']:.1%}")
                print(f"    - Retour moyen: {data['avg_return']:.2%}")
                print(f"    - VolatilitÃ©: {data['std_return']:.2%}")
        
        print(f"\nğŸ¯ PRÃ‰CISION PAR TYPE DE RECOMMANDATION:")
        if "error" not in rec_accuracy:
            for rec_type, data in rec_accuracy.items():
                if data["count"] > 0:
                    print(f"  â€¢ {rec_type}: {data['count']} opportunitÃ©s")
                    print(f"    - PrÃ©cision 1j: {data.get('accuracy_1d', 0):.1%}")
                    print(f"    - Retour moyen 1j: {data.get('avg_return_1d', 0):.2%}")
        
        print(f"\nğŸ’¡ INSIGHTS CLÃ‰S:")
        for finding in insights["key_findings"]:
            print(f"  â€¢ {finding}")
        
        print(f"\nğŸ“‹ RECOMMANDATIONS:")
        for rec in insights["recommendations"]:
            print(f"  â€¢ {rec}")
        
        print("\n" + "="*80)


def main():
    """Fonction principale"""
    
    logger.info("ğŸš€ DÃ©marrage de l'analyse de l'impact Phase 2")
    
    db = next(get_db())
    
    try:
        # CrÃ©er l'analyseur
        analyzer = Phase2ImpactAnalyzer(db)
        
        # ExÃ©cuter l'analyse complÃ¨te
        results = analyzer.run_complete_analysis()
        
        # Afficher le rÃ©sumÃ©
        analyzer.print_summary(results)
        
        # Sauvegarder les rÃ©sultats
        output_file = os.path.join(os.path.dirname(__file__), "phase2_impact_analysis.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“ RÃ©sultats d'analyse sauvegardÃ©s dans {output_file}")
        logger.info("âœ… Analyse de l'impact Phase 2 terminÃ©e avec succÃ¨s")
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'analyse: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        db.close()


if __name__ == "__main__":
    main()

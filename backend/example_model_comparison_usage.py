#!/usr/bin/env python3
"""
Exemple d'Utilisation du Framework de Comparaison de Mod√®les
============================================================

Ce script d√©montre comment utiliser le framework de comparaison de mod√®les
pour analyser diff√©rents symboles et s√©lectionner les meilleurs mod√®les.
"""

import sys
import os
import logging
from datetime import datetime, date, timedelta
from pathlib import Path

# Ajouter le chemin du backend au PYTHONPATH
backend_path = Path(__file__).parent
sys.path.insert(0, str(backend_path))

from app.services.model_comparison_service import ModelComparisonService
from app.core.database import get_db

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def example_single_symbol_analysis():
    """Exemple d'analyse d'un symbole unique"""
    logger.info("üîç Exemple d'analyse d'un symbole unique")
    logger.info("=" * 60)
    
    try:
        # Obtenir une session de base de donn√©es
        db = next(get_db())
        
        # Cr√©er le service de comparaison
        service = ModelComparisonService(db)
        
        # Analyser AAPL (Apple)
        symbol = "AAPL"
        logger.info(f"Analyse du symbole: {symbol}")
        
        # 1. Obtenir des recommandations
        logger.info("\n1Ô∏è‚É£ Obtention des recommandations...")
        recommendations = service.get_model_recommendations(symbol)
        
        if recommendations['success']:
            analysis = recommendations['analysis']
            recs = recommendations['recommendations']
            
            logger.info(f"   üìä Analyse du symbole:")
            logger.info(f"      Volatilit√©: {analysis['volatility']:.3f} ({analysis['volatility_class']})")
            logger.info(f"      Tendance: {analysis['trend']:.3f} ({analysis['trend_class']})")
            logger.info(f"      Volume moyen: {analysis['avg_volume']:,.0f}")
            
            logger.info(f"   üéØ Recommandations:")
            logger.info(f"      Mod√®les primaires: {recs['primary']}")
            logger.info(f"      Mod√®les secondaires: {recs['secondary']}")
            logger.info(f"      √Ä √©viter: {recs['avoid']}")
            
            for reason in recs['reasoning']:
                logger.info(f"      üí° {reason}")
        
        # 2. Comparer les mod√®les recommand√©s
        logger.info("\n2Ô∏è‚É£ Comparaison des mod√®les...")
        
        # Utiliser les mod√®les recommand√©s ou par d√©faut
        models_to_test = recommendations['recommendations']['primary'] if recommendations['success'] else ['RandomForest']
        
        # Ajouter quelques mod√®les suppl√©mentaires pour la comparaison
        additional_models = ['LightGBM', 'NeuralNetwork']
        for model in additional_models:
            if model not in models_to_test and model in service.framework.default_models:
                models_to_test.append(model)
        
        logger.info(f"   Mod√®les √† tester: {models_to_test}")
        
        # Comparer les mod√®les
        comparison_result = service.compare_models_for_symbol(
            symbol=symbol,
            models_to_test=models_to_test,
            start_date=date.today() - timedelta(days=365),  # 1 an de donn√©es
            end_date=date.today()
        )
        
        if comparison_result['success']:
            logger.info(f"   ‚úÖ Comparaison r√©ussie!")
            
            # Afficher les r√©sultats d√©taill√©s
            results = comparison_result['results']
            logger.info(f"\n   üìà R√©sultats d√©taill√©s:")
            logger.info(f"   {'Mod√®le':<20} {'Accuracy':<10} {'F1-Score':<10} {'Sharpe':<10} {'Return':<10}")
            logger.info(f"   {'-'*20} {'-'*10} {'-'*10} {'-'*10} {'-'*10}")
            
            for model_name, metrics in results.items():
                logger.info(
                    f"   {model_name:<20} "
                    f"{metrics.accuracy:<10.3f} "
                    f"{metrics.f1_score:<10.3f} "
                    f"{metrics.sharpe_ratio:<10.3f} "
                    f"{metrics.total_return:<10.3f}"
                )
            
            # Afficher le meilleur mod√®le
            best_model = comparison_result['best_model']
            logger.info(f"\n   üèÜ Meilleur mod√®le: {best_model['name']}")
            logger.info(f"      F1-Score: {best_model['metrics']['f1_score']:.3f}")
            logger.info(f"      Accuracy: {best_model['metrics']['accuracy']:.3f}")
            logger.info(f"      Sharpe Ratio: {best_model['metrics']['sharpe_ratio']:.3f}")
            logger.info(f"      Total Return: {best_model['metrics']['total_return']:.3f}")
            
            # Informations sur les donn√©es
            data_info = comparison_result['data_info']
            logger.info(f"\n   üìä Informations sur les donn√©es:")
            logger.info(f"      √âchantillons d'entra√Ænement: {data_info['train_samples']}")
            logger.info(f"      √âchantillons de test: {data_info['test_samples']}")
            logger.info(f"      Nombre de features: {data_info['features']}")
            
        else:
            logger.warning(f"   ‚ö†Ô∏è √âchec de la comparaison: {comparison_result.get('error', 'Unknown error')}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
        return False
    finally:
        if 'db' in locals():
            db.close()

def example_multiple_symbols_analysis():
    """Exemple d'analyse de plusieurs symboles"""
    logger.info("\nüîç Exemple d'analyse de plusieurs symboles")
    logger.info("=" * 60)
    
    try:
        # Obtenir une session de base de donn√©es
        db = next(get_db())
        
        # Cr√©er le service de comparaison
        service = ModelComparisonService(db)
        
        # S√©lectionner quelques symboles populaires
        symbols = ["AAPL", "MSFT", "GOOGL", "TSLA", "NVDA"]
        
        # V√©rifier quels symboles sont disponibles
        available_symbols = service.get_available_symbols()
        symbols_to_test = [s for s in symbols if s in available_symbols]
        
        if not symbols_to_test:
            logger.warning("Aucun des symboles s√©lectionn√©s n'est disponible")
            return False
        
        logger.info(f"Symboles √† analyser: {symbols_to_test}")
        
        # Comparer les mod√®les sur plusieurs symboles
        logger.info("\nüîÑ Comparaison multi-symboles...")
        
        multi_result = service.compare_models_for_multiple_symbols(
            symbols=symbols_to_test,
            models_to_test=['RandomForest', 'LightGBM']  # Limiter pour √©viter les timeouts
        )
        
        if multi_result['success']:
            summary = multi_result['summary']
            logger.info(f"‚úÖ Analyse multi-symboles termin√©e!")
            logger.info(f"   Symboles analys√©s: {summary['successful_symbols']}/{summary['total_symbols']}")
            logger.info(f"   Taux de succ√®s: {summary['success_rate']:.1%}")
            
            # Afficher les victoires par mod√®le
            model_wins = multi_result['model_wins']
            if model_wins:
                logger.info(f"\nüèÜ Victoires par mod√®le:")
                for model, wins in sorted(model_wins.items(), key=lambda x: x[1], reverse=True):
                    logger.info(f"   {model}: {wins} victoire(s)")
            
            # Afficher les m√©triques moyennes
            avg_metrics = multi_result['model_avg_metrics']
            if avg_metrics:
                logger.info(f"\nüìä M√©triques moyennes par mod√®le:")
                logger.info(f"   {'Mod√®le':<15} {'Accuracy':<12} {'F1-Score':<12} {'Sharpe':<12}")
                logger.info(f"   {'-'*15} {'-'*12} {'-'*12} {'-'*12}")
                
                for model_name, metrics in avg_metrics.items():
                    logger.info(
                        f"   {model_name:<15} "
                        f"{metrics['accuracy']['mean']:<12.3f} "
                        f"{metrics['f1_score']['mean']:<12.3f} "
                        f"{metrics['sharpe_ratio']['mean']:<12.3f}"
                    )
            
            # Afficher les symboles qui ont √©chou√©
            failed_symbols = multi_result['failed_symbols']
            if failed_symbols:
                logger.warning(f"\n‚ö†Ô∏è Symboles ayant √©chou√©: {failed_symbols}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse multi-symboles: {e}")
        return False
    finally:
        if 'db' in locals():
            db.close()

def example_custom_model_creation():
    """Exemple de cr√©ation de mod√®les personnalis√©s"""
    logger.info("\nüîß Exemple de cr√©ation de mod√®les personnalis√©s")
    logger.info("=" * 60)
    
    try:
        from app.services.model_comparison_framework import (
            ModelComparisonFramework, 
            RandomForestModel
        )
        
        # Cr√©er le framework
        framework = ModelComparisonFramework()
        
        # Cr√©er des mod√®les personnalis√©s avec diff√©rents param√®tres
        custom_models = {
            'RF_Conservative': RandomForestModel(
                'RF_Conservative',
                n_estimators=50,
                max_depth=5,
                min_samples_split=10,
                min_samples_leaf=5
            ),
            'RF_Aggressive': RandomForestModel(
                'RF_Aggressive',
                n_estimators=300,
                max_depth=20,
                min_samples_split=2,
                min_samples_leaf=1
            ),
            'RF_Balanced': RandomForestModel(
                'RF_Balanced',
                n_estimators=150,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2
            )
        }
        
        # Ajouter les mod√®les au framework
        for name, model in custom_models.items():
            framework.add_model(name, model)
            logger.info(f"‚úÖ Mod√®le {name} ajout√©")
        
        # Tester avec des donn√©es synth√©tiques
        import numpy as np
        np.random.seed(42)
        
        X_train = np.random.randn(500, 15)
        y_train = np.random.randint(0, 2, 500)
        X_test = np.random.randn(100, 15)
        y_test = np.random.randint(0, 2, 100)
        prices = np.random.randn(100) * 100 + 100
        
        # Comparer les mod√®les personnalis√©s
        logger.info("\nüîÑ Comparaison des mod√®les personnalis√©s...")
        
        results = framework.compare_models(
            X_train, y_train, X_test, y_test, prices,
            models_to_test=list(custom_models.keys())
        )
        
        # Afficher les r√©sultats
        logger.info(f"\nüìà R√©sultats des mod√®les personnalis√©s:")
        logger.info(f"   {'Mod√®le':<20} {'Accuracy':<10} {'F1-Score':<10} {'Sharpe':<10}")
        logger.info(f"   {'-'*20} {'-'*10} {'-'*10} {'-'*10}")
        
        for model_name, metrics in results.items():
            logger.info(
                f"   {model_name:<20} "
                f"{metrics.accuracy:<10.3f} "
                f"{metrics.f1_score:<10.3f} "
                f"{metrics.sharpe_ratio:<10.3f}"
            )
        
        # Obtenir le meilleur mod√®le
        best_model_name, best_metrics = framework.get_best_model('f1_score')
        logger.info(f"\nüèÜ Meilleur mod√®le personnalis√©: {best_model_name}")
        logger.info(f"   F1-Score: {best_metrics.f1_score:.3f}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation de mod√®les personnalis√©s: {e}")
        return False

def example_model_recommendations():
    """Exemple d'utilisation des recommandations de mod√®les"""
    logger.info("\nüéØ Exemple d'utilisation des recommandations")
    logger.info("=" * 60)
    
    try:
        # Obtenir une session de base de donn√©es
        db = next(get_db())
        
        # Cr√©er le service de comparaison
        service = ModelComparisonService(db)
        
        # Analyser diff√©rents types de symboles
        symbol_types = {
            'Tech_Giant': 'AAPL',
            'Growth_Stock': 'TSLA', 
            'Value_Stock': 'BRK.B',
            'Volatile_Stock': 'NVDA',
            'Stable_Stock': 'JNJ'
        }
        
        available_symbols = service.get_available_symbols()
        
        for stock_type, symbol in symbol_types.items():
            if symbol not in available_symbols:
                logger.info(f"‚ö†Ô∏è {symbol} non disponible, passage au suivant...")
                continue
            
            logger.info(f"\nüìä Analyse de {symbol} ({stock_type})...")
            
            recommendations = service.get_model_recommendations(symbol)
            
            if recommendations['success']:
                analysis = recommendations['analysis']
                recs = recommendations['recommendations']
                
                logger.info(f"   Volatilit√©: {analysis['volatility']:.3f} ({analysis['volatility_class']})")
                logger.info(f"   Tendance: {analysis['trend']:.3f} ({analysis['trend_class']})")
                logger.info(f"   Recommand√©: {recs['primary']}")
                
                if recs['avoid']:
                    logger.info(f"   √Ä √©viter: {recs['avoid']}")
            else:
                logger.warning(f"   ‚ö†Ô∏è √âchec de l'analyse: {recommendations.get('error', 'Unknown error')}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse des recommandations: {e}")
        return False
    finally:
        if 'db' in locals():
            db.close()

def main():
    """Fonction principale avec exemples d'utilisation"""
    logger.info("üöÄ Exemples d'Utilisation du Framework de Comparaison de Mod√®les")
    logger.info("=" * 80)
    
    examples = [
        ("Analyse d'un symbole unique", example_single_symbol_analysis),
        ("Analyse de plusieurs symboles", example_multiple_symbols_analysis),
        ("Cr√©ation de mod√®les personnalis√©s", example_custom_model_creation),
        ("Utilisation des recommandations", example_model_recommendations),
    ]
    
    results = {}
    
    for example_name, example_func in examples:
        logger.info(f"\n{'='*80}")
        logger.info(f"üß™ {example_name}")
        logger.info(f"{'='*80}")
        
        try:
            result = example_func()
            results[example_name] = result
            
            if result:
                logger.info(f"‚úÖ {example_name}: R√âUSSI")
            else:
                logger.info(f"‚ùå {example_name}: √âCHOU√â")
                
        except Exception as e:
            logger.error(f"üí• {example_name}: ERREUR CRITIQUE - {e}")
            results[example_name] = False
    
    # R√©sum√© des exemples
    logger.info(f"\n{'='*80}")
    logger.info("üìä R√âSUM√â DES EXEMPLES")
    logger.info(f"{'='*80}")
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for example_name, result in results.items():
        status = "‚úÖ R√âUSSI" if result else "‚ùå √âCHOU√â"
        logger.info(f"  {example_name}: {status}")
    
    logger.info(f"\nüéØ R√©sultat global: {passed}/{total} exemples r√©ussis")
    
    if passed == total:
        logger.info("üéâ Tous les exemples sont pass√©s ! Le framework est pr√™t √† √™tre utilis√©.")
    else:
        logger.warning(f"‚ö†Ô∏è {total - passed} exemple(s) ont √©chou√©. V√©rifiez les logs ci-dessus.")
    
    logger.info(f"\nüìö Prochaines √©tapes:")
    logger.info(f"   1. Int√©grer le framework dans le frontend")
    logger.info(f"   2. Ajouter des mod√®les avanc√©s (LSTM, Transformer)")
    logger.info(f"   3. Impl√©menter des m√©thodes d'ensemble")
    logger.info(f"   4. Optimiser les performances pour de gros datasets")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
